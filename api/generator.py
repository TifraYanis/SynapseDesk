import os
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, GenerationConfig

MODEL_NAME = os.getenv("MISTRAL_MODEL", "mistralai/Mistral-7B-Instruct-v0.2")
DEVICE = os.getenv("CUDA_DEVICE", "")  # vide = CPU

class Generator:
    """
    G√©n√©rateur de r√©ponses contextuelles bas√© sur Hugging Face.
    Objectif : privil√©gier les informations issues du contexte RAG (docs internes)
    avant d'utiliser les connaissances g√©n√©riques du mod√®le.
    """

    def __init__(self, model_name: str = MODEL_NAME):
        self.model_name = model_name
        print(f"[Generator] Initialisation du mod√®le : {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
        self.model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)
        self.pipe = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            device_map="auto" if DEVICE != "" else None
        )

    def synthesize(self, question: str, contexts: list, max_new_tokens: int = 256) -> str:
        """
        G√©n√®re une r√©ponse √† partir d'une question et d'un ensemble de contextes.
        Les contextes sont fournis par le moteur RAG (documents internes les plus pertinents).
        """
        prompt = self.build_prompt(question, contexts)
        gencfg = GenerationConfig(max_new_tokens=max_new_tokens, temperature=0.2, top_p=0.9)
        outputs = self.pipe(prompt, max_new_tokens=max_new_tokens, do_sample=False, generation_config=gencfg)
        return outputs[0]["generated_text"]

    def build_prompt(self, question: str, contexts: list) -> str:
        """
        Construit le prompt en combinant les documents internes et la question utilisateur.
        Objectif : guider le mod√®le √† prioriser les informations d‚Äôentreprise.
        """
        # Concat√®ne les extraits contextuels du RAG
        if contexts:
            ctx_block = "\n\n---\n\n".join(
                [f"üìÑ Source ({c['path']}):\n{c['text']}" for c in contexts]
            )
        else:
            ctx_block = "(Aucun document interne trouv√©, r√©ponse bas√©e sur la connaissance g√©n√©rale du mod√®le.)"

        # Prompt clair et hi√©rarchis√©
        prompt = f"""
Tu es un assistant virtuel d'entreprise fiable et professionnel.
Ta mission est d'aider les collaborateurs (data, RH, support, produit, etc.) √† trouver
des r√©ponses claires en utilisant d'abord les informations internes disponibles.

Question pos√©e :
{question}

Contexte interne r√©cup√©r√© :
{ctx_block}

Consignes :
1. Si les documents internes contiennent des √©l√©ments pertinents, fonde ta r√©ponse dessus.
2. Sinon, r√©ponds de mani√®re neutre et professionnelle √† partir de tes connaissances g√©n√©rales.
3. Structure la r√©ponse avec ces sections :
   - **R√©sum√© clair**
   - **D√©tails ou explications**
   - **Actions / Prochaines √©tapes**
   - **Sources internes cit√©es (si disponibles)**

R√©ponse :
"""
        return prompt.strip()
