import streamlit as st
import requests
import os
import time
from streamlit_extras.colored_header import colored_header

API_URL = os.environ.get("API_URL", "http://127.0.0.1:8000")

st.set_page_config(page_title="Tests RAG", page_icon="ğŸ§ ", layout="wide")

# --- En-tÃªte ---
colored_header(
    label="ğŸ§ª ScÃ©narios de test â€“ RAG Ops Copilot",
    description="Explorez diffÃ©rents profils et types de questions pour visualiser comment le moteur RAG combine les donnÃ©es internes et les connaissances gÃ©nÃ©rales.",
    color_name="violet-70"
)

# --- DÃ©finition des scÃ©narios enrichis ---
scenarios = {
    "ğŸ‘·â€â™‚ï¸ Data Engineer": [
        "Pourquoi mon job bronze_to_silver est lent ?",
        "Comment corriger un OOM sur un job PySpark ?",
        "Quelle est la bonne pratique pour gÃ©rer les joins volumineux ?",
    ],
    "ğŸ§‘â€ğŸ’¼ Product Owner": [
        "Quels sont les indicateurs clÃ©s du projet Data Platform ?",
        "Comment suivre les performances des jobs Spark ?",
    ],
    "ğŸ‘©â€ğŸ’» RH / Manager": [
        "Quelle est la procÃ©dure d'onboarding dâ€™un nouveau collaborateur data ?",
        "Comment contacter lâ€™Ã©quipe Data Platform ?",
    ],
    "ğŸ“Š Conseiller MÃ©tier": [
        "Comment analyser les logs dâ€™un job Ã©chouÃ© ?",
        "Que faire en cas de ticket incident rÃ©current ?",
    ],
    "ğŸŒ GÃ©nÃ©ral / LLM pur": [
        "Câ€™est quoi Spark et Ã  quoi Ã§a sert ?",
        "Quelle est la capitale du Japon ?"
    ]
}

# --- Interface ---
st.write("### Choisissez un profil et testez une question :")

profil = st.selectbox("SÃ©lectionnez un profil :", list(scenarios.keys()))
questions = scenarios[profil]
selected_question = st.selectbox("Choisissez une question :", questions)

if st.button("ğŸš€ Lancer le test", use_container_width=True):
    t0 = time.time()
    try:
        r = requests.post(f"{API_URL}/query", json={"query": selected_question}, timeout=300)
        latency = time.time() - t0
        if not r.ok:
            st.error(f"Erreur API ({r.status_code}): {r.text}")
        else:
            data = r.json()
            st.success(f"RÃ©ponse reÃ§ue en {latency:.2f} s âœ…")
            st.markdown(f"### ğŸ§  Question : *{selected_question}*")
            st.markdown("---")

            st.markdown("#### ğŸ’¬ RÃ©ponse :")
            st.markdown(data.get("answer", "_Aucune rÃ©ponse_"))

            if data.get("citations"):
                st.markdown("#### ğŸ“š Sources RAG :")
                for c in data["citations"]:
                    with st.expander(f"ğŸ“„ {c['source']} (score {round(c['score'],3)})"):
                        st.code(c.get("snippet", ""), language="text")
            else:
                st.info("â„¹ï¸ Aucune source interne trouvÃ©e, rÃ©ponse purement LLM.")
    except Exception as e:
        st.error(f"Erreur de communication avec lâ€™API : {e}")

st.markdown("---")
st.markdown("""
### ğŸ§© Notes :
- Les questions "Data Engineer" / "PO" / "RH" / "Conseiller" activent le **RAG** (recherche sur les fichiers internes).
- Les questions "GÃ©nÃ©ral / LLM pur" sont lÃ  pour tester le **fallback vers Ollama seul**.
- Vous pouvez observer les temps de rÃ©ponse et la prÃ©sence (ou non) de citations internes.
""")
