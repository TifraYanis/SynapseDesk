import os
import json
from pathlib import Path
import streamlit as st
import yaml

# --------------------------------
# Config page
# --------------------------------
st.set_page_config(page_title="Pr√©sentation", page_icon="üìò", layout="wide")
st.markdown("<h2 class='page-title'>üìò Pr√©sentation du projet</h2>", unsafe_allow_html=True)

# Helper badge
def badge(text, color="#0ea5e9"):
    st.markdown(
        f"""
        <span style="
            display:inline-block;
            padding:4px 10px;
            border-radius:999px;
            background:{color};
            color:white;
            font-size:0.85rem;
            margin-right:8px;">{text}</span>
        """,
        unsafe_allow_html=True,
    )

# Charger settings.yaml si pr√©sent
settings_path = Path("configs/settings.yaml")
settings_text = settings_path.read_text(encoding="utf-8") if settings_path.exists() else "Fichier de config introuvable."
try:
    settings_yaml = yaml.safe_load(settings_text) if settings_path.exists() else {}
except Exception:
    settings_yaml = {}

# R√©sum√© haut de page
with st.container():
    col_l, col_r = st.columns([2, 1])
    with col_l:
        st.write(
            """
            **SynapseDesk** est un RAG local qui connecte la connaissance interne de l'entreprise √† un mod√®le de langage.
            L'objectif est de fournir des r√©ponses pertinentes, sourc√©es et actionnables, en privil√©giant d'abord vos donn√©es.
            """
        )
        badge("100% local")
        badge("RAG hybride")
        badge("Ollama + Mistral", "#10b981")
        badge("FAISS + BM25", "#8b5cf6")
        badge("Re-ranking CrossEncoder", "#f59e0b")
        st.caption("Techniques cl√©s: FastAPI pour l'API, Streamlit pour le front de d√©mo.")
    with col_r:
        st.metric("Corpus", value="api/output/corpus.jsonl", delta="index√© en FAISS+BM25")
        st.metric("LLM", value=os.getenv("OLLAMA_MODEL", "mistral:latest"))
        st.metric("Adaptation", value="RAG ou Chat libre")

st.divider()

# --------------------------------
# Onglets
# --------------------------------
tabs = st.tabs([
    "üîé 1. Vue d'ensemble",
    "üóÇÔ∏è 2. Ingestion",
    "üß≠ 3. Indexation et Retrieval",
    "üéØ 4. Re-ranking",
    "üß† 5. Synth√®se LLM",
    "‚öñÔ∏è 6. Logique d‚Äôadaptation",
    "üß™ 7. Tests et Validation",
    "‚öôÔ∏è 8. Configuration et D√©ploiement",
    "üó∫Ô∏è 9. Sch√©ma global",
])

# --------------------------------
# 1. Vue d'ensemble
# --------------------------------
with tabs[0]:
    st.subheader("Objectif du produit")
    st.markdown(
        """
        - Offrir un copilote interne pour les √©quipes Data, IT, RH, m√©tiers et support.  
        - R√©pondre aux questions en **priorisant les sources d'entreprise**.  
        - Expliquer, diagnostiquer, proposer des **actions concr√®tes** et **citer les sources**.  
        """
    )

    st.subheader("Pipeline logique")
    st.markdown(
        """
        1. Ingestion de documents h√©t√©rog√®nes  
        2. Indexation hybride: embeddings denses (FAISS) et lexical (BM25)  
        3. Retrieval des candidats  
        4. Re-ranking par CrossEncoder  
        5. Construction du contexte et synth√®se LLM  
        6. Adaptation: si pas de contexte pertinent, bascule en chat libre  
        """
    )

    st.subheader("üîç Comprendre FAISS et BM25")
    col1, col2 = st.columns(2)
    with col1:
        st.markdown(
            """
            **üìò FAISS (Facebook AI Similarity Search)**  
            - Librairie d√©velopp√©e par Meta pour la **recherche vectorielle**.  
            - Permet de comparer des textes selon leur **proximit√© s√©mantique**,  
              c‚Äôest-√†-dire le **sens global** plut√¥t que les mots exacts.  
            - Dans SynapseDesk, chaque passage de document est encod√© en **vecteur num√©rique (embedding)**  
              via le mod√®le `intfloat/multilingual-e5-base`.  
            - FAISS permet ensuite de retrouver les passages **les plus proches** du sens de la question.  
            """
        )
    with col2:
        st.markdown(
            """
            **üìó BM25 (Best Matching 25)**  
            - M√©thode classique de **recherche lexicale** issue du monde des moteurs de recherche (Okapi BM25).  
            - Elle compare la **fr√©quence et la raret√©** des mots d‚Äôune requ√™te dans chaque document.  
            - Tr√®s utile pour les **termes techniques exacts, acronymes, noms de jobs ou scripts internes**.  
            - Dans SynapseDesk, BM25 compl√®te FAISS en apportant une vision **mots-cl√©s et fr√©quence**.  
            - En combinant FAISS + BM25, on obtient une **recherche hybride** :  
              robuste aux reformulations et sensible aux termes exacts.  
            """
        )

    with st.expander("Pourquoi ce design"):
        st.write(
            """
            - BM25 est robuste aux mots cl√©s, acronymes et fautes mineures.  
            - FAISS capture la similarit√© s√©mantique et les paraphrases.  
            - Le CrossEncoder rerank les candidats en tenant compte de la question.  
            - Le LLM reste au service des donn√©es internes.  
            - La bascule en chat libre √©vite d'injecter du bruit si les scores sont faibles.  
            """
        )

# ------------------------------
# 2. Ingestion des donn√©es internes
# ------------------------------
with tabs[1]:
    st.subheader("R√¥le de l‚Äôingestion")
    st.markdown(
        """
        L‚Äô√©tape d‚Äôingestion transforme les diff√©rentes sources internes de l‚Äôentreprise
        (documents, logs, notes, codes) en un corpus unifi√© et exploitable pour le moteur de recherche.

        Cette phase est r√©alis√©e par le script **`api/ingest_data.py`**, qui :
        - parcourt le dossier `api/data/`,
        - lit et convertit plusieurs formats (Markdown, TXT, HTML, DOCX, JSON, code Python),
        - nettoie les balises et m√©tadonn√©es inutiles,
        - sauvegarde le tout dans un corpus **`api/output/corpus.jsonl`** utilisable par la suite.
        """
    )

    col1, col2 = st.columns(2)
    with col1:
        st.markdown("**Structure des donn√©es**")
        st.code(
            """
            api/
              data/
                Troubleshooting_Spark.md
                Onboarding_Policy.md
                Incident_OOM.docx
                Confluence_DataPlatform.txt
                log_0007.json
              output/
                corpus.jsonl
            """,
            language="text",
        )
        st.caption("Chaque ligne du corpus repr√©sente un document normalis√© (texte brut + m√©tadonn√©es).")

    with col2:
        st.markdown("**Extrait simplifi√© du code**")
        st.code(
            """
            for file in os.listdir(data_dir):
                if file.endswith(('.md', '.txt', '.docx', '.html', '.json')):
                    text = extract_text(file)
                    record = {"text": text, "path": file}
                    corpus.append(record)
            save_jsonl(corpus, "api/output/corpus.jsonl")
            """,
            language="python",
        )
        st.caption("Chaque fichier est lu, converti et ajout√© au corpus unifi√©.")

    with st.expander("Bonnes pratiques d‚Äôingestion"):
        st.write(
            """
            - Nettoyer les fichiers HTML avant ingestion.  
            - Supprimer les menus, banni√®res et m√©tadonn√©es inutiles.  
            - Centraliser les exports Confluence, politiques RH et logs techniques.  
            - Identifier les formats non textuels (PDF, images OCR) pour les futurs ajouts.
            """
        )

    st.info(
        "üí° Cette √©tape ne r√©alise **pas de chunking** ‚Äî chaque document est index√© dans son ensemble. "
        "Cela suffit pour les fichiers courts et moyens de ton corpus. "
        "Le d√©coupage en chunks pourra √™tre ajout√© plus tard pour les documents longs (option d‚Äôam√©lioration).",
        icon="üß©"
    )


# --------------------------------
# 3. Indexation & Retrieval
# --------------------------------

with tabs[2]:
    st.subheader("Cr√©ation de l‚Äôindex hybride (FAISS + BM25)")
    st.markdown(
    """
    Cette √©tape est assur√©e par le script scripts/build_hybrid_index.py,
    qui lit le corpus api/output/corpus.jsonl (g√©n√©r√© par l‚Äôingestion) et produit
    deux structures d‚Äôindex compl√©mentaires :
    - un index FAISS pour la recherche vectorielle s√©mantique,
    - un corpus BM25 pour la recherche lexicale bas√©e sur les mots-cl√©s.
    """
    )

    col1, col2 = st.columns(2)
    with col1:
        st.markdown("**Encodage des documents avec FAISS**")
        st.code(
            """
            # scripts/build_hybrid_index.py
            model = SentenceTransformer("intfloat/multilingual-e5-base")
            embs = model.encode(texts, normalize_embeddings=True, convert_to_numpy=True)
            index = faiss.IndexFlatIP(embs.shape[1])
            index.add(embs)
            faiss.write_index(index, "./indices/dense.index")
            """,
            language="python",
        )
        st.caption("Chaque document est transform√© en vecteur dense et ajout√© √† l‚Äôindex FAISS (similarit√© cosinus).")
    with col2:
        st.markdown("**Construction du corpus BM25**")
        st.code(
            """
            with open("./indices/bm25_corpus.txt", "w", encoding="utf-8") as f:
                for r in records:
                    f.write(" ".join(tokenize(r["text"])) + "\\n")
            """,
            language="python",
        )
        st.caption("Les textes sont tokenis√©s pour permettre une recherche lexicale robuste.")

    st.markdown("**Fusion FAISS + BM25 lors du Retrieval (api/retriever.py)**")
    st.code(
        """
        combined = {}
        for i, cid in enumerate(dense_ids):
            combined[cid] = combined.get(cid, 0) + (10 - i)
        for i, cid in enumerate(bm25_ids):
            combined[cid] = combined.get(cid, 0) + (10 - i)
        candidate_ids = sorted(combined, key=combined.get, reverse=True)[:30]
        """,
        language="python",
    )

    st.markdown("**Comportement r√©el dans le projet :**")
    st.write(
        """
        - Le Retriever recherche d‚Äôabord dans les deux espaces (FAISS et BM25).  
        - Il fusionne les r√©sultats avec un score pond√©r√© pour prioriser les passages communs.  
        - Ces passages candidats sont ensuite envoy√©s au **CrossEncoder** pour re-ranking.  
        """
    )

    with st.expander("Pourquoi FAISS + BM25 dans SynapseDesk"):
        st.write(
            """
            - **BM25** capture les mots exacts, acronymes et noms de processus internes.  
            - **FAISS** comprend les formulations vari√©es ou traduites.  
            - Leur **fusion** assure robustesse et couverture compl√®te, essentielle en environnement h√©t√©rog√®ne.  
            """
        )

# --------------------------------

# 4. Re-ranking

# --------------------------------

with tabs[3]:
    st.subheader("√âtape de Re-ranking avec CrossEncoder")
    st.markdown(
    """
    Une fois les candidats r√©cup√©r√©s par FAISS et BM25, il est n√©cessaire de **r√©√©valuer leur pertinence**
    en tenant compte de la question utilisateur.
    Cette √©tape est cruciale pour √©viter que le LLM lise des passages approximatifs ou non prioritaires.
    Elle repose sur un mod√®le **CrossEncoder**, qui pr√©dit un score de similarit√© *question ‚Üî passage*.
    """
    )

    st.markdown("**Principe du CrossEncoder**")
    st.write(
        """
        - Le mod√®le prend **chaque paire (question, passage)** comme entr√©e.  
        - Il calcule un **score de correspondance** bas√© sur le contexte combin√©, contrairement aux embeddings
        o√π les textes sont encod√©s s√©par√©ment.  
        - Plus le score est √©lev√©, plus le passage est jug√© pertinent.  
        - Les meilleurs passages (top-k) sont ensuite transmis √† l‚Äô√©tape de g√©n√©ration du LLM.
        """
    )

    st.code(
        """
        # api/retriever.py
        from sentence_transformers import CrossEncoder

        cross = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

        pairs = [(query, records[i]["text"]) for i in candidate_ids]
        rerank_scores = cross.predict(pairs)

        scored = []
        for i, cid in enumerate(candidate_ids):
            scored.append({
                "id": cid,
                "score": float(rerank_scores[i]),
                "text": records[cid]["text"],
                "path": records[cid]["path"],
            })

        hits = sorted(scored, key=lambda x: x["score"], reverse=True)[:8]
        """,
        language="python",
    )

    st.markdown("**Pourquoi cette √©tape est indispensable dans SynapseDesk**")
    st.write(
        """
        - üîç Le CrossEncoder permet d‚Äô√©liminer les faux positifs issus de FAISS ou BM25.  
        - ‚öôÔ∏è Il prend en compte la s√©mantique compl√®te de la question et du passage ensemble.  
        - üí¨ Il am√©liore la coh√©rence des r√©ponses en orientant le LLM vers les sources vraiment pertinentes.  
        - ‚è±Ô∏è M√™me s‚Äôil est plus co√ªteux, il reste viable localement gr√¢ce au mod√®le compact `MiniLM-L-6-v2`.  
        """
    )

    with st.expander("Notes techniques et optimisation"):
        st.write(
            """
            - On limite le **nombre de passages rerank√©s (ex : 30 ‚Üí 8)** pour r√©duire la latence.  
            - Le mod√®le CrossEncoder est charg√© une seule fois lors du d√©marrage de l‚ÄôAPI FastAPI.  
            - Des variantes multilingues peuvent √™tre utilis√©es si le corpus contient des textes anglais/fran√ßais.  
            - Pour des environnements GPU, l‚Äôutilisation de `device_map='auto'` acc√©l√®re fortement le scoring.  
            """
        )


# --------------------------------

# 5. Synth√®se LLM

# --------------------------------

with tabs[4]:
    st.subheader("Synth√®se de la r√©ponse avec le LLM (Mistral via Ollama)")
    st.markdown(
    """
    Apr√®s la s√©lection et le classement des passages les plus pertinents,
    la derni√®re √©tape consiste √† **construire un prompt clair et structur√©**
    pour le mod√®le de langage, afin qu‚Äôil g√©n√®re une r√©ponse **fiable, concise et sourc√©e**.
    """
    )

    st.markdown("**üß© Construction du prompt**")
    st.code(
        """
        ctx_text = "\\n\\n---\\n\\n".join([f"Source ({c['path']}):\\n{c['text']}" for c in contexts])

        prompt = f\"\"\"
        Tu es un assistant d'entreprise sp√©cialis√© dans le support aux √©quipes techniques et fonctionnelles.
        Utilise les informations suivantes pour r√©pondre √† la question :

        Question : {q}

        Contexte :
        {ctx_text}

        Structure ta r√©ponse avec :
        - R√©sum√©
        - Analyse
        - Recommandations concr√®tes
        - Sources (liste)
        \"\"\"
        """,
        language="python",
    )
    st.caption("Chaque passage du contexte est concat√©n√© pour cr√©er un prompt complet et tra√ßable.")

    # ---- Ollama ----
    st.subheader("üöÄ Ex√©cution locale via Ollama")
    st.write(
        """
        **Ollama** est une plateforme open source qui permet d‚Äôex√©cuter des mod√®les de langage (LLM) 
        **en local**, sans d√©pendance cloud.  
        Il g√®re le t√©l√©chargement, la quantification (compression GPU/CPU), 
        et l‚Äôex√©cution efficace des mod√®les comme **Mistral**, **Llama 3**, ou **Gemma**.
        """
    )

    st.markdown("Appel Ollama via `/api/generate` (Mistral local)")
    st.code(
        """
        # api/llm_manager.py
        OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")
        OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "mistral:7b")

        payload = {
            "model": OLLAMA_MODEL,
            "prompt": prompt,
            "stream": False
        }

        r = requests.post(f"{OLLAMA_URL}/api/generate", json=payload, timeout=120)
        answer = r.json().get("response", "")
        """,
        language="python",
    )

    st.markdown("**Comment √ßa marche**")
    st.write(
        """
        1. Le backend FastAPI envoie le prompt √† l‚ÄôAPI d‚ÄôOllama (`/api/chat`).  
        2. Ollama charge le mod√®le sp√©cifi√© (par d√©faut `mistral:latest`).  
        3. Le mod√®le g√©n√®re le texte r√©ponse localement, sans connexion externe.  
        4. La r√©ponse est renvoy√©e √† Streamlit pour affichage, avec les citations associ√©es.  
        """
    )

    # ---- Pourquoi Mistral ----
    st.subheader("üß† Pourquoi Mistral ?")
    st.write(
        """
        **Mistral 7B** (version *Instruct*) a √©t√© choisi pour son √©quilibre entre **performance, taille et localit√©** :
        - Mod√®le compact (7 milliards de param√®tres) performant sur CPU/GPU modestes.  
        - Excellente compr√©hension du fran√ßais et du vocabulaire technique (data, IT, RH‚Ä¶).  
        - Ouvert, rapide et bien support√© par Ollama.  
        - Tr√®s faible co√ªt mat√©riel compar√© √† GPT ou Claude.  
        """
    )

    st.markdown("**Alternatives possibles**")
    st.write(
        """
        - ü¶ô **Llama 3 (8B / 13B)** ‚Üí plus pr√©cis, mais plus lourd.  
        - üßÆ **Phi-3-mini (Microsoft)** ‚Üí tr√®s l√©ger, id√©al CPU.  
        - üî¨ **Gemma 2 (Google)** ‚Üí excellent en raisonnement, mais encore en test sur Ollama.  
        - üåç **Mixtral 8x7B** ‚Üí version Mixture-of-Experts du mod√®le Mistral, plus puissante mais n√©cessite GPU ‚â• 12 Go.  
        """
    )

    with st.expander("D√©tails techniques de l'int√©gration Mistral/Ollama"):
        st.write(
            """
            - L‚Äôappel √† Ollama se fait via HTTP local (`localhost:11434`).  
            - Le mod√®le reste en m√©moire (gr√¢ce √† `OLLAMA_KEEP_ALIVE`) pour des r√©ponses instantan√©es.  
            - La temp√©rature et le `top_p` peuvent √™tre ajust√©s dans `llm_manager.py` pour moduler la cr√©ativit√©.  
            - En cas d‚Äôerreur ou de service Ollama inactif, un **fallback automatique** vers HuggingFace peut √™tre ajout√©.  
            """
        )


# --------------------------------

# 6. Logique d‚Äôadaptation

# --------------------------------

with tabs[5]:
    st.subheader("Logique d‚Äôadaptation : quand activer ou non le RAG")
    st.markdown(
    """
    Dans **SynapseDesk**, le syst√®me ne se contente pas d‚Äôinjecter les passages trouv√©s √† chaque question.
    Il **√©value dynamiquement la pertinence moyenne** des documents rerank√©s, afin de choisir entre deux modes :

    ```
        - üß© **Mode RAG** : les sources internes sont jug√©es fiables et int√©gr√©es dans la r√©ponse.  
        - üí¨ **Mode Chat libre** : les r√©sultats sont trop faibles, le LLM r√©pond seul sans contexte.
    """
    )

    st.markdown("**üîé Principe de d√©tection automatique**")
    st.write(
        """
        Le syst√®me v√©rifie le **score maximal** parmi les passages rerank√©s.  
        Si ce score est trop faible (inf√©rieur √† `0.01`) et que la requ√™te semble bien une question,
        le mod√®le consid√®re que le contexte RAG serait peu utile et bascule en **chat libre**.
        """
    )

    st.code(
        """
        # api/main.py
        max_score = max(h["score"] for h in hits)

        if max_score < 0.01 and "?" in q:
            answer = generate_with_ollama(f"R√©ponds clairement √† : {q}", max_tokens=512)
            return {"answer": answer, "citations": []}
        else:
            # RAG normal avec les passages les plus pertinents
        """,
        language="python",
    )

    st.markdown("**üéõ Param√©trage possible**")
    st.write(
        """
        - Le **seuil de d√©clenchement (0.01)** est d√©fini dans `api/main.py`.  
        - Un seuil plus haut rend la d√©tection plus stricte (le RAG s‚Äôactive moins souvent).  
        - La condition `"?" in q` permet d‚Äô√©viter d‚Äôactiver le mode libre sur des entr√©es non interrogatives.  
        - Cette approche garantit que le LLM ne r√©ponde librement **que lorsqu‚Äôaucune source interne n‚Äôest pertinente**.
        """
    )

    st.markdown("**Pourquoi cette logique est essentielle**")
    st.write(
        """
        - ‚úÖ √âvite les r√©ponses pollu√©es par des logs ou documents hors sujet.  
        - üß† R√©duit le risque d‚Äôhallucination contextuelle.  
        - üîç Am√©liore la lisibilit√© des r√©sultats pour les utilisateurs non techniques.  
        - ‚öñÔ∏è Offre un √©quilibre entre pr√©cision (RAG) et flexibilit√© (chat libre).  
        """
    )

    with st.expander("Sch√©ma conceptuel de la bascule automatique"):
            st.graphviz_chart(
                """
                digraph Adaptation {
                rankdir=LR;
                node [shape=box, style=filled, color="#1E1E2E", fontcolor="white", fillcolor="#2A2A3B"];
                edge [color="#8AA1B1"];

                Q [label="Question utilisateur"];
                R [label="Retrieval + Re-ranking"];
                S [label="√âvaluation du score maximal"];
                RAG [label="Mode RAG actif (sources internes)"];
                CHAT [label="Mode Chat libre (sans contexte)"];

                Q -> R -> S;
                S -> RAG [label="max_score ‚â• 0.01", color="#10b981"];
                S -> CHAT [label="max_score < 0.01", color="#ef4444"];
                }
                """
            )

    with st.expander("Exemples pratiques"):
        st.write(
            """
            - üß† *Question technique claire* ‚Üí "Pourquoi mon job Spark met 10 minutes √† s‚Äôex√©cuter ?"  
            ‚Üí score √©lev√©, activation du RAG (r√©ponse bas√©e sur les logs internes).  
            - üí¨ *Question vague ou RH* ‚Üí "Comment va l‚Äô√©quipe ce mois-ci ?"  
            ‚Üí score faible, passage en mode chat libre avec r√©ponse g√©n√©rique.  
            """
        )


# --------------------------------

# 7. Tests et Validation

# --------------------------------

with tabs[6]:
    st.subheader("Tests et validation du syst√®me RAG")
    st.markdown(
    """
    Une fois le pipeline complet en place, il est essentiel de **valider la coh√©rence, la pr√©cision et la robustesse**
    du mod√®le RAG. Les tests garantissent que le syst√®me comprend bien les questions,
    s√©lectionne des sources pertinentes et produit des r√©ponses fiables.
    """
    )

    st.markdown("**üéØ Types de tests men√©s dans SynapseDesk**")
    st.write(
        """
        - **Questions de r√©f√©rence techniques** : validation sur des jobs Spark, erreurs OOM, ou scripts internes.  
        - **Cas non techniques** : documents RH, politiques internes, fiches m√©tiers.  
        - **Cas limites** : acronymes, abr√©viations, fautes d‚Äôorthographe ou expressions multilingues.  
        - **Mesures quantitatives** :  
        - Temps de r√©ponse API (FastAPI + Ollama).  
        - Ratio d‚Äôactivation du mode RAG vs. chat libre.  
        - Taux de citations exactes des sources (path + snippet).  
        """
    )

    st.subheader("üß™ Exemple de test rapide via l‚ÄôAPI FastAPI")
    st.code(
        r"""
    ```

    curl -X POST [http://127.0.0.1:8000/query](http://127.0.0.1:8000/query) -H "Content-Type: application/json" ^
    -d "{"query": "Pourquoi mon job bronze_to_silver est lent ?"}"
    """,
    language="bash",
    )

    st.markdown("**Interpr√©tation du r√©sultat**")
    st.write(
        """
        - Si le message `‚ÑπÔ∏è Aucune source interne trouv√©e` appara√Æt ‚Üí le syst√®me a bascul√© en **chat libre**.  
        - Si plusieurs sources (`data\\...json`) sont list√©es ‚Üí le **RAG est actif**, et la r√©ponse est sourc√©e.  
        - En mode d√©mo, les logs FastAPI affichent :  
        `"[Retriever] Top N r√©sultats apr√®s rerank."`, confirmant le bon fonctionnement du moteur hybride.  
        """
    )

    with st.expander("Bonnes pratiques de validation continue"):
        st.write(
            """
            - Maintenir un **jeu de tests r√©currents** (fichier JSON de Q/A attendues).  
            - Surveiller l‚Äô√©volution du **score moyen** apr√®s chaque mise √† jour du corpus.  
            - √âvaluer r√©guli√®rement la **pertinence per√ßue** via des retours utilisateurs internes.  
            - Int√©grer ces tests dans un workflow CI (GitHub Actions ou Makefile d√©di√©).  
            """
        )

    # --------------------------------

    # 8. Configuration et D√©ploiement

    # --------------------------------

    with tabs[7]:
        st.subheader("üß© Fichier de configuration : settings.yaml")
        st.markdown(
        """
        Le fichier `configs/settings.yaml` centralise les **chemins**, **mod√®les** et **param√®tres de seuil** utilis√©s par le projet.
        Il permet d‚Äôajuster le comportement global sans modifier le code Python.
        """
        )
        st.code(settings_text, language="yaml")
        st.caption("Les param√®tres contr√¥lent notamment : le chemin du corpus, le mod√®le d‚Äôembedding, et les seuils d‚Äôactivation du RAG.")

        st.subheader("‚öôÔ∏è Commandes Makefile principales")
        st.markdown(
            """
            Les principales √©tapes du pipeline sont automatis√©es via le **Makefile**.
            Cela garantit un d√©ploiement rapide et coh√©rent, que ce soit en local ou sur serveur.
            """
        )
        st.code(
            """
            make setup           # Cr√©ation du venv + installation requirements
            make generate-data   # G√©n√®re un petit jeu de donn√©es de test
            make ingest          # Produit api/output/corpus.jsonl (via ingest_data.py)
            make build-index     # Construit l'index FAISS + BM25 √† partir du corpus
            make api             # Lance le backend FastAPI (http://127.0.0.1:8000)
            make streamlit       # Lance l‚Äôinterface Streamlit
            """,
            language="bash",
        )

        st.subheader("üöÄ D√©ploiement et bonnes pratiques")
        st.write(
            """
            - **Versionnement** : conserver une copie du dossier `indices/` et du corpus `api/output/corpus.jsonl` pour rejouer les m√™mes r√©sultats.  
            - **Performance GPU** : ajuster `OLLAMA_NUM_PARALLEL` (sessions concurrentes) et `OLLAMA_KEEP_ALIVE` pour r√©duire la latence.  
            - **Logs d‚Äôusage** : consigner les requ√™tes, temps de r√©ponse et passages cit√©s pour l‚Äôaudit interne.  
            - **S√©curit√©** : filtrer les donn√©es sensibles (PII) avant ingestion, et limiter les acc√®s API √† l‚Äôintranet.  
            - **Extensibilit√©** : pr√©voir un dossier `api/plugins/` pour des retrievers sp√©cialis√©s (RH, incidents, tickets‚Ä¶).  
            """
        )

        with st.expander("üì¶ Exemple de d√©ploiement local complet"):
            st.markdown(
                """
                ```bash
                git clone https://github.com/TifraYanis/synapsedesk
                cd synapsedesk
                make setup
                make ingest
                make build-index
                make api
                make streamlit
                ```
                ‚Üí Acc√®s local √† l‚ÄôAPI : [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)  
                ‚Üí Interface Streamlit : [http://localhost:8501](http://localhost:8501)
                """
            )

# --------------------------------

# 9. Sch√©ma global

# --------------------------------

with tabs[8]:
    st.subheader("üó∫Ô∏è Sch√©ma global du pipeline SynapseDesk")
    st.markdown(
    """
    Le diagramme ci-dessous illustre le **flux complet de traitement** des requ√™tes dans SynapseDesk :
    de la collecte de donn√©es internes jusqu‚Äô√† la r√©ponse structur√©e du mod√®le Mistral.
    """
    )

    st.graphviz_chart(
        """
        digraph G {
        rankdir=LR;
        node [shape=box, style=filled, color="#1E1E2E", fontcolor="white", fillcolor="#2A2A3B"];
        edge [color="#8AA1B1"];

        A [label="Sources internes (md, txt, html, docx, logs, code)"];
        B [label="Ingestion & Normalisation (ingest_data.py)"];
        C [label="Embeddings (E5 - dense)"];
        D [label="Index FAISS"];
        E [label="Index BM25"];
        Q [label="Question utilisateur"];
        R [label="Fusion candidats (dense + BM25)"];
        X [label="Re-ranking CrossEncoder"];
        CTX [label="Top-k context g√©n√©r√©"];
        LLM [label="LLM local (Ollama + Mistral)"];
        OUT [label="R√©ponse structur√©e + Citations"];
        F [label="Fallback chat libre (si max_score < 0.01)"];

        A -> B -> C -> D;
        B -> E;
        Q -> R;
        D -> R;
        E -> R;
        R -> X -> CTX -> LLM -> OUT;
        X -> F [style=dashed, label="max_score faible (< 0.01)"];
        }
        """
    )


    st.markdown(
            """
            ### üîÑ Lecture du pipeline
            1. **Ingestion & Normalisation** : tous les fichiers internes (`api/data/`) sont lus et convertis en texte brut via `ingest_data.py`.  
            ‚Üí Les formats pris en charge incluent `.md`, `.txt`, `.html`, `.docx`, `.json`, et certains scripts `.py`.  
            2. **Indexation hybride** :  
            - **FAISS** encode chaque texte avec des **embeddings s√©mantiques** (`intfloat/multilingual-e5-base`).  
            - **BM25** conserve une **indexation lexicale** (mots-cl√©s exacts, acronymes, noms de jobs, etc.).  
            3. **Retrieval & Fusion** : les r√©sultats FAISS et BM25 sont combin√©s avec pond√©ration pour produire une liste unifi√©e de passages candidats.  
            4. **Re-ranking** : un mod√®le CrossEncoder (`cross-encoder/ms-marco-MiniLM-L-6-v2`) reclasse les passages selon leur pertinence r√©elle vis-√†-vis de la question.  
            5. **Synth√®se** : le mod√®le **Mistral 7B** (ex√©cut√© localement via **Ollama**) r√©dige une r√©ponse structur√©e : *R√©sum√©, Analyse, Recommandations, Sources*.  
            6. **Fallback automatique** : si le **score maximal** des passages est inf√©rieur √† `0.01`, le syst√®me consid√®re qu‚Äôaucune source interne n‚Äôest pertinente et bascule en **chat libre**.  
            """
        )

    st.info(
        "üí° Utilisez l‚Äôonglet **Tests** pour observer le comportement du pipeline et voir quand le RAG s‚Äôactive ou non.",
        icon="üîç"
    )
